# -*- coding: utf-8 -*-
"""Yet another copy of Narayana(MCTGNN)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14RE9XKdMPEtEFi4sU-SE69XvpRM6aS2y
"""

!pip install torch_geometric kagglehub networkx matplotlib pandas numpy torchdiffeq

# -*- coding: utf-8 -*-
"""OmNamoAranganatha - Cell 1: Data Loading and Electrode Graph Creation"""

import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import kagglehub
import os

# Function to create a fully connected adjacency matrix
def create_fully_connected_adjacency_matrix(n_electrodes):
    adj_matrix = np.ones((n_electrodes, n_electrodes), dtype=int)
    np.fill_diagonal(adj_matrix, 0)  # No self-loops
    return adj_matrix

# Function to plot the 3D electrode graph
def plot_electrode_graph(adj_matrix, coords):
    G = nx.from_numpy_array(adj_matrix)
    pos = {i: (coords.iloc[i]['x'], coords.iloc[i]['y'], coords.iloc[i]['z']) for i in range(len(coords))}
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')
    xs, ys, zs = coords['x'], coords['y'], coords['z']
    scatter = ax.scatter(xs, ys, zs, c='b', marker='o', s=50, label='Electrodes')
    for i in range(len(coords)):
        ax.text(coords.iloc[i]['x'], coords.iloc[i]['y'], coords.iloc[i]['z'], f"E{i}", fontsize=8)
    for edge in G.edges():
        i, j = edge
        ax.plot([coords.iloc[i]['x'], coords.iloc[j]['x']],
                [coords.iloc[i]['y'], coords.iloc[j]['y']],
                [coords.iloc[i]['z'], coords.iloc[j]['z']],
                c='gray', alpha=0.1)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.legend()
    plt.title('Electrode Graph with Labels (162 Electrodes)')
    plt.savefig('electrode_graph.png')
    plt.show()
    print("Saved electrode graph to 'electrode_graph.png'")

# Load and clean coordinates
print("Downloading electrode coordinates dataset...")
path = kagglehub.dataset_download("arunramponnambalam/electrodes-coordinates")
print("Path to dataset files:", path)
dataset_files = os.listdir(path)
print("Files in dataset:", dataset_files)
coords_file = os.path.join(path, 'electrodesdata.csv')
coords = pd.read_csv(coords_file, usecols=[0, 1, 2], names=['x', 'y', 'z'], header=0)

# Remove corrupted electrodes
corrupted_indices = [0, 16, 23, 24, 26, 31, 38, 63, 92, 96, 99, 100, 101, 102, 108, 111, 112, 113, 114, 122, 128, 129, 139, 142, 145, 146, 147, 148, 170, 177, 192, 193]
valid_indices = [i for i in range(len(coords)) if i not in corrupted_indices]
coords = coords.iloc[valid_indices].reset_index(drop=True)
print(f"Removed {len(corrupted_indices)} corrupted electrodes. Remaining: {len(coords)}")

if len(coords) != 162:
    raise ValueError(f"Expected 162 electrodes after cleaning, but found {len(coords)}")

# Create adjacency matrix and visualize
adj_matrix = create_fully_connected_adjacency_matrix(n_electrodes=162)
plot_electrode_graph(adj_matrix, coords)
print(f"Adjacency matrix shape: {adj_matrix.shape}")
print(f"Number of connections: {np.sum(adj_matrix) // 2}")

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import kagglehub
import os
import logging
import matplotlib.pyplot as plt
from joblib import Parallel, delayed
import warnings
import gc
from scipy.interpolate import CubicSpline
import time
from sklearn.decomposition import PCA
from torchdiffeq import odeint
import seaborn as sns
import glob

# Logging and device setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"CUDA Device: {torch.cuda.get_device_name(0)}")
    # Optimizations for A100 GPU
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_tf32 = True
else:
    print("CUDA is not available. Will attempt to continue with CPU but performance will be slow.")

# Constants - optimized for neural dynamics on A100 GPU
NUM_NODES = 162
IN_CHANNELS = 1
MAX_SEQ_LENGTH = 5000  # Can be higher on A100
BATCH_SIZE = 2        # Increased for A100 GPU
LEARNING_RATE = 0.0005
EPOCHS = 50
HIDDEN_DIM = 64        # Increased for better representation capacity
SUBSAMPLE_FACTOR = 10  # Balance between accuracy and speed
MEMORY_ENABLED = True  # Keep memory state enabled as specified

# Create results directory
os.makedirs("results", exist_ok=True)
os.makedirs("results/visualizations", exist_ok=True)
os.makedirs("results/checkpoints", exist_ok=True)
os.makedirs("results/logs", exist_ok=True)

# Helper Functions
def parse_numeric_string(s):
    """Parse string representations of numeric arrays from the dataset."""
    try:
        if not isinstance(s, str):
            return None
        s = s.strip('[]').replace('\n', ' ').replace('...', '0').strip()
        if not s or s.isspace():
            return None
        values = [float(x) for x in s.split() if x.strip() and x.replace('.', '').replace('-', '').isdigit()]
        return np.array(values) if values else None
    except Exception as e:
        logger.warning(f"Failed to parse string: {s[:30]}... - Error: {str(e)}")
        return None

# Dataset Class with improved progress tracking and memory efficiency
class NeuralSignalDataset(Dataset):
    """
    Enhanced dataset for processing neural signal data with efficient chunking
    based on activity patterns and robust normalization.
    """
    def __init__(self, data_df, coords_df, max_seq_length=MAX_SEQ_LENGTH, normalize=True,
                 chunk_min_size=20, max_chunks=130):
        self.data = data_df
        self.coords = coords_df
        self.max_seq_length = max_seq_length
        self.normalize = normalize
        self.chunk_min_size = chunk_min_size
        self.max_chunks = max_chunks
        # List of valid electrode indices (removing problematic electrodes)
        self.valid_indices = [i for i in range(194) if i not in [0, 16, 23, 24, 26, 31, 38, 63, 92, 96, 99, 100, 101, 102, 108, 111, 112, 113, 114, 122, 128, 129, 139, 142, 145, 146, 147, 148, 170, 177, 192, 193]]
        self.process_data()

    def process_data(self):
        """Process the neural data into chunks based on stimulus changes."""
        print("Chunking data dynamically based on stimulus changes...")
        total_rows = len(self.data)
        chunked_data = []
        actual_lengths = []
        current_activity = None
        current_chunk = []

        # Setup progress bar
        progress_bar = tqdm(range(total_rows), desc="Processing rows")

        # Process row by row with original chunking logic
        processed_count = 0
        skipped_count = 0
        chunk_count = 0

        for idx in progress_bar:
            try:
                electrode_str = self.data.iloc[idx]['data']
                electrode_values = parse_numeric_string(electrode_str)
                if electrode_values is None or len(electrode_values) != 194:
                    skipped_count += 1
                    continue
                electrode_values = electrode_values[self.valid_indices]
                if len(electrode_values) != NUM_NODES:
                    skipped_count += 1
                    continue

                activity = self.data.iloc[idx]['activity']
                processed_count += 1

                if current_activity is None:
                    current_activity = activity
                    current_chunk = [electrode_values]
                elif activity != current_activity:
                    if len(current_chunk) > self.chunk_min_size and len(chunked_data) < self.max_chunks:
                        chunked_data.append(np.vstack(current_chunk))
                        actual_lengths.append(len(current_chunk))
                        chunk_count += 1
                    current_chunk = [electrode_values]
                    current_activity = activity
                else:
                    current_chunk.append(electrode_values)

                # Update progress bar description with stats
                if idx % 100 == 0:
                    progress_bar.set_postfix({
                        'processed': processed_count,
                        'skipped': skipped_count,
                        'chunks': chunk_count,
                        'current_chunk_size': len(current_chunk)
                    })

            except Exception as e:
                logger.error(f"Error processing row {idx}: {str(e)}")
                skipped_count += 1
                continue

        # Process the last chunk if needed
        if current_chunk and len(current_chunk) > self.chunk_min_size and len(chunked_data) < self.max_chunks:
            chunked_data.append(np.vstack(current_chunk))
            actual_lengths.append(len(current_chunk))
            chunk_count += 1

        if not chunked_data:
            raise ValueError("No valid data chunks processed.")

        print(f"Processing complete! Found {processed_count} valid rows, created {chunk_count} chunks")

        # Setup progress bar for padding and processing
        print("Preparing final dataset...")
        self.electrode_data = []
        self.actual_lengths = []

        for chunk_idx, (chunk, actual_length) in enumerate(zip(chunked_data, actual_lengths)):
            print(f"Processing chunk {chunk_idx+1}/{len(chunked_data)}, length={actual_length}")
            # Trim to max sequence length first if needed
            if actual_length > self.max_seq_length:
                chunk = chunk[:self.max_seq_length]
                actual_length = self.max_seq_length

            # Apply padding if needed
            pad_len = self.max_seq_length - actual_length
            padded_chunk = np.pad(chunk, ((0, pad_len), (0, 0)), mode='constant')

            self.electrode_data.append(padded_chunk)
            self.actual_lengths.append(actual_length)

        self.electrode_data = np.stack(self.electrode_data)
        print(f"Stacked data shape before normalization: {self.electrode_data.shape}")

        if self.normalize:
            print("Normalizing data...")
            try:
                scaler = StandardScaler()
                # Process in batches to save memory
                batch_size = 1000
                shape = self.electrode_data.shape
                reshaped = self.electrode_data.reshape(-1, NUM_NODES)

                for start_idx in range(0, reshaped.shape[0], batch_size):
                    end_idx = min(start_idx + batch_size, reshaped.shape[0])
                    if start_idx == 0:
                        # Fit on first batch
                        scaler.partial_fit(reshaped[start_idx:end_idx])
                    else:
                        # Update fit with subsequent batches
                        scaler.partial_fit(reshaped[start_idx:end_idx])

                # Transform in batches
                normalized = np.zeros_like(reshaped)
                for start_idx in range(0, reshaped.shape[0], batch_size):
                    end_idx = min(start_idx + batch_size, reshaped.shape[0])
                    normalized[start_idx:end_idx] = scaler.transform(reshaped[start_idx:end_idx])

                self.electrode_data = normalized.reshape(shape)

            except Exception as e:
                logger.error(f"Error during normalization: {str(e)}")
                # If standard scaling fails, use a simpler approach
                mean = np.mean(self.electrode_data)
                std = np.std(self.electrode_data) + 1e-8
                self.electrode_data = (self.electrode_data - mean) / std

        print(f"Dataset preparation complete! Final shape: {self.electrode_data.shape}")

    def __len__(self):
        return len(self.electrode_data)

    def __getitem__(self, idx):
        sequences = self.electrode_data[idx]
        actual_length = self.actual_lengths[idx]
        return torch.FloatTensor(sequences).unsqueeze(-1), torch.tensor(actual_length, dtype=torch.long)


# Optimized ODE Function with memory state
class MemoryODEFunc(nn.Module):
    """
    ODE function that models neural dynamics with both activity state and memory state.

    This dual-state approach allows the model to capture both the immediate neural
    activity and the accumulated context over time.
    """
    def __init__(self, hidden_dim, adj_matrix):
        super(MemoryODEFunc, self).__init__()
        self.hidden_dim = hidden_dim
        self.adj_matrix = adj_matrix

        # Pre-compute normalized adjacency
        node_degrees = self.adj_matrix.sum(dim=1).view(-1, 1) + 1e-8  # Avoid division by zero
        self.normalized_adj = self.adj_matrix / node_degrees

        # Main network for activity dynamics
        self.activity_net = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),  # Takes both activity and memory
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),  # Smooth activations better for ODEs
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()  # Bounded activation for stability
        )

        # Network for memory dynamics
        self.memory_net = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),  # Takes both activity and memory
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Sigmoid()  # Memory gating with sigmoid
        )

        # Graph attention mechanism for spatial influence
        self.graph_attention = nn.Sequential(
            nn.Linear(hidden_dim, 1),
            nn.Softmax(dim=1)  # Attention across nodes
        )

        # Learnable parameters for dynamics control
        self.diffusion_scale = nn.Parameter(torch.Tensor([0.01]))
        self.memory_decay = nn.Parameter(torch.Tensor([0.05]))
        self.time_scale = nn.Parameter(torch.Tensor([1.0]))

    def spatial_diffusion(self, x, batch_size):
        """
        Compute spatial diffusion across the graph using normalized adjacency matrix.
        Uses attention mechanism to weight node influences.
        """
        # Compute attention weights for diffusion
        attention = self.graph_attention(x)  # Shape: [batch, nodes, 1]

        # Apply attention to node features
        x_weighted = x * attention

        # Graph diffusion with normalized adjacency
        diffusion = torch.bmm(
            self.normalized_adj.float().unsqueeze(0).expand(batch_size, -1, -1),
            x_weighted
        )

        return diffusion * self.diffusion_scale

    def forward(self, t, z):
        """
        Compute the derivatives for both activity and memory states.

        Args:
            t: Current time point
            z: Combined state tensor [batch_size, num_nodes, 2*hidden_dim]
               where first half is activity state and second half is memory state

        Returns:
            dz_dt: Derivatives for both states
        """
        batch_size, num_nodes, total_dim = z.shape

        # Split into activity and memory states
        x = z[:, :, :self.hidden_dim]  # Activity state
        m = z[:, :, self.hidden_dim:]  # Memory state

        # Handle any NaN values
        x = torch.nan_to_num(x)
        m = torch.nan_to_num(m)

        # Combine states for input to neural networks
        z_combined = torch.cat([x, m], dim=-1)

        # Compute spatial diffusion for activity state
        diffusion = self.spatial_diffusion(x, batch_size)

        # Compute activity dynamics with diffusion influence
        dx_dt = self.activity_net(z_combined) + diffusion - self.memory_decay * x

        # Compute memory update (slower dynamics, integrating activity)
        dm_dt = self.memory_net(z_combined) * (x - m)

        # Combine derivatives
        dz_dt = torch.cat([dx_dt, dm_dt], dim=-1)

        # Apply time scaling factor
        dz_dt = self.time_scale * dz_dt

        # Gradient clipping for numerical stability
        norm = torch.norm(dz_dt, dim=-1, keepdim=True)
        max_norm = 10.0
        scale = torch.clamp(max_norm / (norm + 1e-8), max=1.0)
        dz_dt = dz_dt * scale

        # Final check for NaN values
        dz_dt = torch.nan_to_num(dz_dt)

        return dz_dt


# Neural ODE model with stability features
class NeuralODEWithMemory(nn.Module):
    """
    Neural ODE model that integrates both activity and memory states to model
    neural dynamics over time.

    This model solves the ODE system defined by MemoryODEFunc and handles
    the integration with robust error handling and fallback mechanisms.
    """
    def __init__(self, in_channels, hidden_dim, adj_matrix, memory_enabled=MEMORY_ENABLED):
        super(NeuralODEWithMemory, self).__init__()
        self.hidden_dim = hidden_dim
        self.adj_matrix = adj_matrix
        self.memory_enabled = memory_enabled
        self.integration_points = None  # Will be set during training

        # Encoder for initial state
        self.encoder = nn.Sequential(
            nn.Linear(in_channels, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim * 2 if memory_enabled else hidden_dim)
        )

        # ODE function
        self.ode_func = MemoryODEFunc(hidden_dim, adj_matrix)

        # Decoder for predictions
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, in_channels)
        )

        # Default solver method
        self.solver = "dopri5"  # Options: "dopri5", "implicit_adams", "rk4", "euler"

    def torchdiffeq_integration(self, z0, integration_time):
        """
        Integration using torchdiffeq's odeint function with appropriate solver.

        Args:
            z0: Initial state
            integration_time: Time points for integration

        Returns:
            solution: Integrated trajectory
        """
        try:
            if self.solver == "dopri5":
                solution = odeint(
                    self.ode_func,
                    z0,
                    integration_time,
                    method='dopri5',
                    rtol=1e-3,
                    atol=1e-4,
                    options={'dtype': torch.float32}
                )
            elif self.solver == "implicit_adams":
                solution = odeint(
                    self.ode_func,
                    z0,
                    integration_time,
                    method='implicit_adams',
                    rtol=1e-2,
                    atol=1e-3,
                    options={'max_num_steps': 4000}
                )
            elif self.solver == "rk4":
                solution = odeint(
                    self.ode_func,
                    z0,
                    integration_time,
                    method='rk4',
                    options={'step_size': 0.1}
                )
            else:  # default to euler
                solution = odeint(
                    self.ode_func,
                    z0,
                    integration_time,
                    method='euler',
                    options={'step_size': 0.05}
                )

            # Check for NaN values
            if torch.isnan(solution).any():
                print("Warning: NaNs in torchdiffeq integration. Falling back to manual Euler.")
                return self.custom_euler_integration(z0, integration_time)

            return solution

        except Exception as e:
            print(f"Error in torchdiffeq integration: {str(e)}. Using custom Euler method.")
            return self.custom_euler_integration(z0, integration_time)

    def custom_euler_integration(self, z0, integration_time):
        """
        Custom Euler integration method for stability and consistent dimensions.

        Args:
            z0: Initial state
            integration_time: Time points for integration

        Returns:
            solution: Integrated trajectory
        """
        num_time_points = len(integration_time)
        batch_size, num_nodes, state_dim = z0.shape
        device = z0.device

        # Initialize solution array
        solution = torch.zeros(num_time_points, batch_size, num_nodes, state_dim, device=device)
        solution[0] = z0

        # Calculate time steps
        dt = integration_time[1:] - integration_time[:-1]

        # Integration loop
        for i in range(num_time_points - 1):
            t = integration_time[i]
            current_state = solution[i]

            try:
                # Calculate derivative
                derivative = self.ode_func(t, current_state)

                # Apply Euler step with stability measures
                next_state = current_state + derivative * dt[i].item()

                # Check for NaNs or Infs
                if torch.isnan(next_state).any() or torch.isinf(next_state).any():
                    # Use stabilized update
                    derivative_clipped = torch.clamp(derivative, min=-10.0, max=10.0)
                    next_state = current_state + derivative_clipped * min(dt[i].item(), 0.1)
                    next_state = torch.nan_to_num(next_state)

                solution[i + 1] = next_state

            except Exception as e:
                print(f"Error in step {i}: {str(e)}. Using fallback.")
                # Fallback: small random step
                solution[i + 1] = current_state + 0.001 * torch.randn_like(current_state)

            # Clear cache periodically
            if i % 20 == 0:
                torch.cuda.empty_cache()

        return solution

    def interpolate_solution(self, solution, integration_time, seq_len):
        """
        Interpolate the ODE solution to the full sequence length.

        Args:
            solution: Integrated trajectory at integration time points
            integration_time: Time points used for integration
            seq_len: Full sequence length to interpolate to

        Returns:
            full_solution: Interpolated solution at all time points
        """
        try:
            # Create full time points
            full_time = torch.arange(seq_len, device=solution.device).float()

            # Using linear interpolation for robustness
            interp_solution = torch.zeros(seq_len, *solution.shape[1:], device=solution.device)

            # Find corresponding indices and interpolate
            for i, t in enumerate(full_time):
                # Find adjacent integration points
                idx = torch.sum(integration_time <= t).clamp(1, len(integration_time) - 1).item()
                idx_prev = idx - 1

                # Get interpolation weights
                t_prev = integration_time[idx_prev]
                t_next = integration_time[idx]
                alpha = (t - t_prev) / (t_next - t_prev + 1e-8)

                # Linear interpolation
                interp_solution[i] = (1 - alpha) * solution[idx_prev] + alpha * solution[idx]

            return interp_solution

        except Exception as e:
            print(f"Error in interpolation: {str(e)}. Using simple copying.")
            # Simple fallback - copy nearest neighbor
            interp_solution = torch.zeros(seq_len, *solution.shape[1:], device=solution.device)
            for i in range(seq_len):
                # Find nearest integration point
                nearest_idx = (torch.abs(integration_time - i)).argmin().item()
                interp_solution[i] = solution[nearest_idx]

            return interp_solution

    def forward(self, x):
        """
        Forward pass of the model.

        Args:
            x: Input tensor [batch_size, seq_len, num_nodes, channels]

        Returns:
            pred_sequences: Predicted sequences
            solution: Full ODE solution (for analysis)
        """
        batch_size, seq_len, num_nodes, channels = x.shape

        # Get initial state from the first time step
        initial_x = x[:, 0, :, :]
        z0 = self.encoder(initial_x)

        # For memory model, split initial state if needed
        if self.memory_enabled and z0.shape[-1] != self.hidden_dim * 2:
            # Initialize both activity and memory
            z0_activity = z0
            z0_memory = torch.zeros_like(z0)
            z0 = torch.cat([z0_activity, z0_memory], dim=-1)

        # Create integration time points based on current settings
        if self.integration_points is None:
            self.integration_points = seq_len // SUBSAMPLE_FACTOR + 1

        integration_time = torch.linspace(0, seq_len-1, self.integration_points, device=device)

        # Solve ODE system
        try:
            # Integration with torchdiffeq
            solution = self.torchdiffeq_integration(z0, integration_time)

            # Interpolate to full sequence
            full_solution = self.interpolate_solution(solution, integration_time, seq_len)

            # Extract activity state for prediction (first half of state dimension)
            if self.memory_enabled:
                pred_x = full_solution[:, :, :, :self.hidden_dim]
            else:
                pred_x = full_solution

            # Apply decoder to get predictions
            pred_x_flat = pred_x.reshape(-1, self.hidden_dim)
            pred_obs_flat = self.decoder(pred_x_flat)
            pred_obs = pred_obs_flat.reshape(seq_len, batch_size, num_nodes, channels)

            # Permute to match expected shape [batch_size, seq_len, num_nodes, channels]
            pred_sequences = pred_obs.permute(1, 0, 2, 3)

            return pred_sequences, full_solution

        except Exception as e:
            print(f"Error in forward pass: {str(e)}. Using emergency fallback.")

            # Emergency fallback - simple dynamics
            pred_sequences = torch.zeros_like(x)
            full_solution = torch.zeros(seq_len, batch_size, num_nodes,
                                      self.hidden_dim * 2 if self.memory_enabled else self.hidden_dim,
                                      device=device)

            # Copy input as prediction and add small noise
            pred_sequences[:, 1:] = x[:, :-1] + 0.01 * torch.randn_like(x[:, :-1])

            return pred_sequences, full_solution


# Enhanced model with GRU context accumulation
class ContextualNeuralODE(nn.Module):
    """
    Enhanced neural ODE model that combines ODE dynamics with GRU-based
    contextual processing to capture multi-scale temporal dependencies.
    """
    def __init__(self, in_channels, hidden_dim, adj_matrix, memory_enabled=MEMORY_ENABLED):
        super(ContextualNeuralODE, self).__init__()

        # Neural ODE component
        self.neural_ode = NeuralODEWithMemory(in_channels, hidden_dim, adj_matrix, memory_enabled)

        # GRU for context accumulation across time
        self.context_rnn = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=1,
            batch_first=True
        )

        # Attention mechanism for contextual integration
        self.time_attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

        # Final prediction layer
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, in_channels)
        )

    def forward(self, x):
        """
        Forward pass of the contextual neural ODE model.

        Args:
            x: Input tensor [batch_size, seq_len, num_nodes, channels]

        Returns:
            predictions: Predicted sequences
            solution: ODE solution for analysis
        """
        batch_size, seq_len, num_nodes, channels = x.shape

        try:
            # Get ODE predictions and solution
            ode_predictions, solution = self.neural_ode(x)

            # Extract activity states from solution
            if self.neural_ode.memory_enabled:
                activity_states = solution[:, :, :, :self.neural_ode.hidden_dim]
            else:
                activity_states = solution

            # Process each node separately with GRU
            context_enhanced = []

            for node_idx in range(num_nodes):
                # Extract this node's activity across all batches and time
                node_activity = activity_states[:, :, node_idx, :].permute(1, 0, 2)  # [batch, time, hidden]

                # Apply GRU for temporal context
                try:
                    context_output, _ = self.context_rnn(node_activity)

                    # Combine ODE state with GRU context using attention
                    combined_features = torch.cat([node_activity, context_output], dim=-1)
                    attention_scores = self.time_attention(combined_features)
                    attention_weights = F.softmax(attention_scores, dim=1)

                    # Apply attention to get final representation
                    context_node = node_activity * (1 + attention_weights)

                except Exception as e:
                    print(f"Error in GRU for node {node_idx}: {str(e)}. Using fallback.")
                    context_node = node_activity  # Fallback to just ODE output

                context_enhanced.append(context_node)

            # Stack all nodes
            context_enhanced = torch.stack(context_enhanced, dim=2)  # [batch, time, nodes, hidden]

            # Predict final output
            context_flat = context_enhanced.reshape(-1, self.neural_ode.hidden_dim)
            ode_flat = ode_predictions.reshape(-1, channels)

            context_pred_flat = self.predictor(
                torch.cat([context_flat, ode_flat.repeat(1, self.neural_ode.hidden_dim)], dim=-1)
            )

            final_predictions = context_pred_flat.reshape(batch_size, seq_len, num_nodes, channels)

            # Weighted combination of ODE and context predictions
            alpha = 0.7  # Weight for ODE predictions
            predictions = alpha * ode_predictions + (1 - alpha) * final_predictions

            return predictions, solution

        except Exception as e:
            print(f"Error in contextual model: {str(e)}. Falling back to ODE only.")
            return self.neural_ode(x)


# Function to create correlation-based adjacency matrix
def create_correlation_adjacency_matrix(data, valid_indices, coords=None, n_samples=10000, threshold=0.5):
    """
    Create an adjacency matrix based on spatial correlations in neural activity.

    Args:
        data: DataFrame containing neural data
        valid_indices: List of valid electrode indices
        coords: DataFrame with electrode coordinates
        n_samples: Number of samples to use for correlation calculation
        threshold: Correlation threshold for edge creation

    Returns:
        adj_matrix: PyTorch tensor containing adjacency matrix
    """
    print(f"Computing correlation adjacency matrix using {n_samples} samples...")

    # Check if we should use coordinates instead (faster)
    if coords is not None and (n_samples > len(data) or n_samples > 5000):
        print("Using coordinate-based adjacency matrix (faster than correlation)...")
        return create_coordinate_adjacency_matrix(coords, threshold)

    # Process samples in batches to save memory
    batch_size = 500
    num_batches = (n_samples + batch_size - 1) // batch_size

    # Initialize correlation matrix accumulator
    num_valid_indices = len(valid_indices)
    correlation_sum = np.zeros((num_valid_indices, num_valid_indices))
    samples_processed = 0

    for batch in tqdm(range(num_batches), desc="Computing correlation matrix"):
        start_idx = batch * batch_size
        end_idx = min(start_idx + batch_size, n_samples, len(data))
        batch_samples = []

        # Get batch data
        for idx in range(start_idx, end_idx):
            if idx >= len(data):
                break

            try:
                electrode_str = data.iloc[idx]['data']
                electrode_values = parse_numeric_string(electrode_str)
                if electrode_values is not None and len(electrode_values) == 194:
                    batch_samples.append(electrode_values[valid_indices])
            except Exception as e:
                continue

        if not batch_samples:
            continue

        # Process batch
        batch_data = np.array(batch_samples).T
        batch_corr = np.corrcoef(batch_data)
        batch_corr = np.nan_to_num(batch_corr)

        # Add to accumulator
        correlation_sum += batch_corr
        samples_processed += 1

    # Average the correlation matrices
    if samples_processed == 0:
        raise ValueError("No valid batches processed for correlation matrix")

    avg_corr = correlation_sum / samples_processed

    # Create adjacency matrix
    print(f"Creating adjacency matrix with threshold {threshold}...")
    adj_matrix = (np.abs(avg_corr) > threshold).astype(np.float32)
    np.fill_diagonal(adj_matrix, 0)  # No self-loops

    edges = adj_matrix.sum()
    print(f"Created adjacency matrix with {edges} edges")

    return torch.FloatTensor(adj_matrix).to(device)


def create_coordinate_adjacency_matrix(coords, threshold=0.3):
    """
    Create an adjacency matrix based on spatial proximity of electrodes.
    This is faster than correlation-based approach.

    Args:
        coords: DataFrame with electrode coordinates
        threshold: Distance threshold for edge creation

    Returns:
        adj_matrix: PyTorch tensor containing adjacency matrix
    """
    print("Creating coordinate-based adjacency matrix...")

    # Extract coordinates
    coords_array = coords.values
    num_nodes = len(coords_array)

    # Calculate Euclidean distances between electrodes
    distances = np.zeros((num_nodes, num_nodes))
    for i in range(num_nodes):
        for j in range(num_nodes):
            distances[i, j] = np.sqrt(np.sum((coords_array[i] - coords_array[j]) ** 2))

    # Normalize distances to [0, 1]
    max_dist = np.max(distances)
    norm_distances = distances / max_dist

    # Create adjacency matrix based on distance threshold
    adj_matrix = (norm_distances < threshold).astype(np.float32)
    np.fill_diagonal(adj_matrix, 0)  # No self-loops

    edges = adj_matrix.sum()
    print(f"Created coordinate-based adjacency matrix with {edges} edges")

    return torch.FloatTensor(adj_matrix).to(device)


# Training function with curriculum learning
def train_with_curriculum(model, train_loader, epochs=EPOCHS, lr=LEARNING_RATE,
                         curriculum_stages=3, scheduler_type="cosine"):
    """
    Train the model using curriculum learning approach.

    Args:
        model: Model to train
        train_loader: DataLoader for training data
        epochs: Total number of epochs
        lr: Learning rate
        curriculum_stages: Number of curriculum stages
        scheduler_type: Type of learning rate scheduler

    Returns:
        model: Trained model
        history: Training history
    """
    print(f"Starting curriculum learning with {curriculum_stages} stages...")
    device = next(model.parameters()).device

    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
        weight_decay=1e-5,
        eps=1e-8
    )

    # Learning rate scheduler
    if scheduler_type == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=epochs // curriculum_stages,
            T_mult=2,
            eta_min=lr / 10
        )
    else:
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )

    # Track best model and training history
    best_loss = float('inf')
    best_model_state = None
    patience = 7
    patience_counter = 0

    history = {
        'epoch_losses': [],
        'validation_losses': [],
        'learning_rates': [],
        'solver_methods': [],
        'integration_points': []
    }

    # ODE solver curriculum - increasing complexity
    solver_curriculum = ["euler", "rk4", "dopri5", "implicit_adams"]

    # Integration points curriculum - increasing resolution
    if isinstance(model, ContextualNeuralODE):
        ode_model = model.neural_ode
    else:
        ode_model = model

    max_points = MAX_SEQ_LENGTH // SUBSAMPLE_FACTOR + 1
    min_points = max(max_points // 3, 5)
    points_curriculum = np.linspace(min_points, max_points, curriculum_stages).astype(int)

    # Training loop with curriculum
    global_epoch = 0
    training_time_start = time.time()

    for stage in range(curriculum_stages):
        # Update curriculum settings for this stage
        stage_epochs = epochs // curriculum_stages

        # Set integration points for this stage
        ode_model.integration_points = points_curriculum[stage]

        # Set solver for this stage
        solver_idx = min(stage, len(solver_curriculum) - 1)
        ode_model.solver = solver_curriculum[solver_idx]

        print(f"\n===== Curriculum Stage {stage+1}/{curriculum_stages} =====")
        print(f"Integration points: {ode_model.integration_points}")
        print(f"Solver method: {ode_model.solver}")

        # Train for this curriculum stage
        for epoch in range(stage_epochs):
            epoch_loss = 0.0
            batch_count = 0

            # Set to training mode
            model.train()

            # Training loop with progress bar
            train_pbar = tqdm(train_loader, desc=f"Epoch {global_epoch+1}/{epochs}")

            for sequences, actual_lengths in train_pbar:
                try:
                    # Move data to device
                    sequences = sequences.to(device)
                    actual_lengths = actual_lengths.to(device)

                    # Zero gradients
                    optimizer.zero_grad()

                    # Forward pass
                    try:
                        predictions, _ = model(sequences)
                    except Exception as e:
                        print(f"Error in forward pass: {str(e)}")
                        continue

                    # Create mask for actual sequence lengths
                    batch_size, seq_len, num_nodes, channels = sequences.shape
                    mask = torch.zeros(batch_size, seq_len, device=device)
                    for b in range(batch_size):
                        mask[b, :actual_lengths[b]] = 1

                    # Calculate loss with masking and stability
                    try:
                        # MSE loss with masking
                        mse_loss = ((predictions - sequences) ** 2 * mask.unsqueeze(-1).unsqueeze(-1)).sum() / \
                                  (mask.sum() * num_nodes * channels + 1e-8)

                        # Add temporal smoothness loss for stability
                        if seq_len > 1:
                            smoothness_loss = ((predictions[:, 1:] - predictions[:, :-1]) ** 2).mean() * 0.01
                            loss = mse_loss + smoothness_loss
                        else:
                            loss = mse_loss
                    except Exception as e:
                        print(f"Error in loss calculation: {str(e)}")
                        continue

                    # Skip problematic losses
                    if torch.isnan(loss) or torch.isinf(loss):
                        print("Warning: Invalid loss, skipping batch")
                        continue

                    # Backward pass and optimization
                    try:
                        loss.backward()

                        # Gradient clipping
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                        # Update weights
                        optimizer.step()
                    except Exception as e:
                        print(f"Error in backward pass: {str(e)}")
                        continue

                    # Update stats
                    epoch_loss += loss.item()
                    batch_count += 1

                    # Update progress bar
                    train_pbar.set_postfix({'loss': loss.item()})

                    # Periodically clear cache
                    if batch_count % 5 == 0:
                        torch.cuda.empty_cache()

                except Exception as e:
                    print(f"Error during batch: {str(e)}")
                    continue

            # End of epoch processing
            if batch_count > 0:
                avg_loss = epoch_loss / batch_count
            else:
                avg_loss = float('inf')

            # Quick validation
            model.eval()
            val_loss = validate_model(model, train_loader, max_batches=3)

            # Update learning rate
            if scheduler_type != "cosine":
                scheduler.step(val_loss)
            else:
                scheduler.step()

            # Update history
            history['epoch_losses'].append(avg_loss)
            history['validation_losses'].append(val_loss)
            history['learning_rates'].append(scheduler.get_last_lr()[0])
            history['solver_methods'].append(ode_model.solver)
            history['integration_points'].append(ode_model.integration_points)

            # Log performance
            print(f"Epoch {global_epoch+1}/{epochs}, Loss: {avg_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}")

            # Check for improvement
            if val_loss < best_loss:
                best_loss = val_loss
                best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
                print(f"New best model saved! Loss: {best_loss:.6f}")
                patience_counter = 0
            else:
                patience_counter += 1
                print(f"No improvement for {patience_counter} epochs")

            # Early stopping
            if patience_counter >= patience:
                print(f"Early stopping after {global_epoch+1} epochs")
                break

            # Save checkpoint
            if (global_epoch + 1) % 5 == 0:
                torch.save({
                    'epoch': global_epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_loss,
                    'val_loss': val_loss,
                    'curriculum_stage': stage,
                    'history': history
                }, f"results/checkpoints/checkpoint_epoch_{global_epoch+1}.pt")

            global_epoch += 1

        # Create and save visualizations after each stage
        with torch.no_grad():
            visualize_model(model, train_loader, f"curriculum_stage_{stage+1}")

    # Training complete
    training_time = time.time() - training_time_start
    print(f"Training completed in {training_time:.2f} seconds")

    # Load best model
    if best_model_state is not None:
        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})
        print(f"Loaded best model with loss: {best_loss:.6f}")

        # Save final model
        torch.save({
            'model_state_dict': model.state_dict(),
            'history': history,
            'best_loss': best_loss
        }, "results/final_model.pt")

    # Plot training history
    plot_training_history(history)

    return model, history


def validate_model(model, data_loader, max_batches=None):
    """
    Validate the model on a subset of data.

    Args:
        model: Model to validate
        data_loader: DataLoader with validation data
        max_batches: Maximum number of batches to use

    Returns:
        val_loss: Validation loss
    """
    model.eval()
    device = next(model.parameters()).device
    total_loss = 0.0
    batch_count = 0

    with torch.no_grad():
        for sequences, actual_lengths in tqdm(data_loader, desc="Validating", total=max_batches):
            if max_batches is not None and batch_count >= max_batches:
                break

            try:
                # Move data to device
                sequences = sequences.to(device)
                actual_lengths = actual_lengths.to(device)

                # Forward pass
                predictions, _ = model(sequences)

                # Calculate masked loss
                batch_size, seq_len, num_nodes, channels = sequences.shape
                mask = torch.zeros(batch_size, seq_len, device=device)
                for b in range(batch_size):
                    mask[b, :actual_lengths[b]] = 1

                # MSE loss with mask
                loss = ((predictions - sequences) ** 2 * mask.unsqueeze(-1).unsqueeze(-1)).sum() / \
                      (mask.sum() * num_nodes * channels + 1e-8)

                if not torch.isnan(loss) and not torch.isinf(loss):
                    total_loss += loss.item()
                    batch_count += 1

            except Exception as e:
                print(f"Error during validation: {str(e)}")
                continue

    if batch_count > 0:
        return total_loss / batch_count
    else:
        return float('inf')


def plot_training_history(history):
    """
    Plot the training history metrics.

    Args:
        history: Dictionary containing training history
    """
    plt.figure(figsize=(15, 12))

    # Plot loss
    plt.subplot(2, 2, 1)
    plt.plot(history['epoch_losses'], label='Training Loss')
    plt.plot(history['validation_losses'], label='Validation Loss')
    plt.yscale('log')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss Evolution')
    plt.legend()
    plt.grid(True)

    # Plot learning rate
    plt.subplot(2, 2, 2)
    plt.plot(history['learning_rates'])
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedule')
    plt.grid(True)

    # Plot integration points
    plt.subplot(2, 2, 3)
    plt.plot(history['integration_points'])
    plt.xlabel('Epoch')
    plt.ylabel('Integration Points')
    plt.title('Curriculum Learning: Integration Points')
    plt.grid(True)

    # Plot solver methods
    plt.subplot(2, 2, 4)
    methods = history['solver_methods']
    unique_methods = list(dict.fromkeys(methods))
    method_indices = [unique_methods.index(m) for m in methods]
    plt.plot(method_indices)
    plt.yticks(range(len(unique_methods)), unique_methods)
    plt.xlabel('Epoch')
    plt.title('Curriculum Learning: Solver Methods')
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('results/training_history.png')
    plt.close()


def visualize_model(model, data_loader, name_suffix="", max_samples=3):
    """
    Create visualizations of model predictions and internal states.

    Args:
        model: Model to visualize
        data_loader: DataLoader with data
        name_suffix: Suffix for filenames
        max_samples: Maximum number of samples to visualize
    """
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        # Get a few samples
        for sample_idx, (sequences, actual_lengths) in enumerate(data_loader):
            if sample_idx >= max_samples:
                break

            # Move to device
            sequences = sequences.to(device)
            actual_lengths = actual_lengths.to(device)

            # Make predictions
            predictions, solution = model(sequences)

            # Convert to numpy for plotting
            sequences_np = sequences.cpu().numpy()
            predictions_np = predictions.cpu().numpy()

            # Plot a few nodes for this sample
            for batch_idx in range(min(2, sequences.shape[0])):
                # Get actual length for this sequence
                length = actual_lengths[batch_idx].item()

                # Select representative nodes to visualize (first, middle, last third)
                for node_idx in [0, NUM_NODES//3, 2*NUM_NODES//3]:
                    plt.figure(figsize=(10, 6))

                    # Plot actual data and predictions
                    actual = sequences_np[batch_idx, :length, node_idx, 0]
                    pred = predictions_np[batch_idx, :length, node_idx, 0]

                    plt.plot(actual, label='Actual', linewidth=2)
                    plt.plot(pred, label='Predicted', linewidth=2, linestyle='--')

                    plt.title(f'Node {node_idx} Dynamics')
                    plt.xlabel('Time Step')
                    plt.ylabel('Activity')
                    plt.legend()
                    plt.grid(True)

                    # Save figure
                    plt.savefig(f'results/visualizations/dynamics_sample{sample_idx}_batch{batch_idx}_node{node_idx}_{name_suffix}.png')
                    plt.close()

                # Visualize memory states if available
                if isinstance(model, ContextualNeuralODE) and model.neural_ode.memory_enabled:
                    # Extract memory from solution
                    memory_states = solution[:length, batch_idx, :, model.neural_ode.hidden_dim:].cpu().numpy()

                    # PCA to visualize memory states
                    pca = PCA(n_components=2)
                    memory_flat = memory_states.reshape(length, -1)
                    memory_pca = pca.fit_transform(memory_flat)

                    plt.figure(figsize=(8, 8))
                    plt.scatter(memory_pca[:, 0], memory_pca[:, 1], c=range(length), cmap='viridis')
                    plt.colorbar(label='Time Step')
                    plt.title('Memory State Trajectory (PCA)')
                    plt.xlabel('PC1')
                    plt.ylabel('PC2')
                    plt.grid(True)
                    plt.savefig(f'results/visualizations/memory_trajectory_sample{sample_idx}_batch{batch_idx}_{name_suffix}.png')
                    plt.close()

                    # Plot memory activity for a few nodes
                    plt.figure(figsize=(12, 8))
                    for i, node_idx in enumerate([0, NUM_NODES//3, 2*NUM_NODES//3]):
                        plt.subplot(3, 1, i+1)
                        node_memory = memory_states[:, node_idx, 0]  # First memory dimension
                        plt.plot(node_memory)
                        plt.title(f'Node {node_idx} Memory Activity')
                        plt.xlabel('Time Step')
                        plt.ylabel('Memory Value')
                        plt.grid(True)
                    plt.tight_layout()
                    plt.savefig(f'results/visualizations/memory_activity_sample{sample_idx}_batch{batch_idx}_{name_suffix}.png')
                    plt.close()


def main():
    """Main function to run the entire pipeline."""
    try:
        print("Starting Neural ODE model for cortical dynamics...")

        # Check for checkpoints to resume training
        checkpoints = glob.glob("results/checkpoints/checkpoint_epoch_*.pt")
        if checkpoints:
            print(f"Found {len(checkpoints)} checkpoints. To resume training, use the latest one.")

        # Data loading
        print("Loading electrode coordinates...")
        coords_path = kagglehub.dataset_download("arunramponnambalam/electrodes-coordinates")
        coords_file = os.path.join(coords_path, "electrodesdata.csv")
        coords = pd.read_csv(coords_file, usecols=[0, 1, 2], names=['x', 'y', 'z'], header=0)
        valid_indices = [i for i in range(len(coords)) if i not in [0, 16, 23, 24, 26, 31, 38, 63, 92, 96, 99, 100, 101, 102, 108, 111, 112, 113, 114, 122, 128, 129, 139, 142, 145, 146, 147, 148, 170, 177, 192, 193]]
        coords = coords.iloc[valid_indices].reset_index(drop=True)

        print("Loading neural activity data...")
        data_path = kagglehub.dataset_download("nocopyrights/image-stimulus-timestamp-data")
        data_file = os.path.join(data_path, "merged_data.csv")
        data = pd.read_csv(data_file)
        print(f"Loaded dataset with {len(data)} samples")

        # Filter data
        data = data[~(data['activity'].str.startswith('sscr', na=False) |
                      data['activity'].str.startswith('sscr2', na=False))].reset_index(drop=True)
        print(f"Dataset after filtering: {len(data)} samples")

        # Create adjacency matrix
        print("Creating adjacency matrix...")
        adj_matrix = create_correlation_adjacency_matrix(data, valid_indices, coords, n_samples=5000)

        # Initialize dataset
        print("Initializing dataset...")
        dataset = NeuralSignalDataset(data, coords, max_seq_length=MAX_SEQ_LENGTH)

        # Free memory
        del data
        gc.collect()
        torch.cuda.empty_cache()

        # Create data loader
        print("Creating data loader...")
        train_loader = DataLoader(
            dataset,
            batch_size=BATCH_SIZE,
            shuffle=True,
            num_workers=0,  # Using 0 workers for stability
            pin_memory=True,
            drop_last=True  # Drop last batch if smaller than batch_size
        )

        # Initialize model
        print("Initializing model...")
        model = ContextualNeuralODE(IN_CHANNELS, HIDDEN_DIM, adj_matrix, memory_enabled=MEMORY_ENABLED).to(device)

        # Print model summary
        total_params = sum(p.numel() for p in model.parameters())
        print(f"Model initialized with {total_params:,} parameters")
        print(f"Memory-enabled: {MEMORY_ENABLED}")

        # Train model
        model, history = train_with_curriculum(
            model,
            train_loader,
            epochs=EPOCHS,
            lr=LEARNING_RATE,
            curriculum_stages=3
        )

        print("Training completed!")

        # Generate final visualizations
        print("Generating final visualizations...")
        visualize_model(model, train_loader, "final")

        print("Done!")

    except Exception as e:
        logger.error(f"Error in main: {str(e)}")
        print(f"Caught exception: {str(e)}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"Error in main: {str(e)}")
        print(f"Caught exception: {str(e)}")

